\section{Goals and  Impact}

\subsection{Overview}
% \para{Overview} 
%
The \name~prototype system will deploy an experimental-management infrastructure that allows users to construct community-wide experiments that implement data and metadata controls on the inclusion and exclusion of data. It will do so by building upon the principle of continuous integration (CI), adopted from the agile software development community. The CI platform allows the designer of an experiment to place multiple simple or complex controls on data, such as requiring specific metadata, that data are registered to a given atlas, or that data are collected using specific experimentation protocols. Controls may include reprocessing data to make it compliant, e.g. reregistering MRI data to a given atlas or band-pass filtering electrophysiology data. Controls and processing will run user-defined scripts and programs within a software container (e.g., Docker) on a serverless computing cloud, such as Amazon Lambda. Containers encapsulate all dependencies to ensure that software runs uniformly against all inputs on all platforms. Serverless computing automatically and elastically provisions cloud resources to run containers.

\name~will create a meaningful way to scale science from the tens of subjects used by individual labs today to thousands or tens of thousands, while allowing scientists to exert controls that ensure data quality, initially with two different experimental patterns: (1) An incremental experiment defines an experiment against a existing data set and then opens the experiment to community contributions. Other labs/scientists contribute data which gets processed on submission and included or rejected. The experiment maintains online dashboards that show how additional data change results with complete provenance. (2) A derived experiment forks/branches an existing experiment allowing a researcher to change properties, such as an acceptance criteria or analysis algorithm, but otherwise run the same pipeline against the same inputs. Full provenance allows scientists to reason and debate about how modifications affect outcomes. These patterns can be composed and the fork/branch version control model of the CI system connects changes in experiments to outcomes. We will co-develop community experiments for MRI and for neurophysiology (including both optical and electrical physiology); these domains were chosen because they are critical to NSF BRAIN neuroscience, have large bodies of unshared data living in silos in different labs, and they produce large numbers of relatively small (GB-scale) data sets that are managed easily in a prototype.

\para{Broader Impacts} 
%
\name~has the potential to permanently transform practice in neuroscience. The design principles for the system grew out of the NSF KAVLI Global Brain Workshop's discussion on realizing a global-shared infrastructure for scientific discovery. \name~overcomes major obstacles to data sharing. It does not restrict scientists to a specific workflow system, ontologies, software frameworks, etc. Rather, scientists package their custom experiments into containers and set admission criteria for new data. Scientists share data without losing control over data quality; they gain full provenance on how all subsequent experiments use their data and algorithms. This provenance can be used to debate and reason about different results from the same data or how specific data changes affect results. With these important barriers to data sharing removed, we envision a system that meaningfully integrates the 1000s of publicly available data resources in MRI and neurophysiology and creates incentives for data sharing for individual labs and data collectors, who gain great analytic power by confronting their new data with a massive corpus.

\para{Intellectual Merit} 
%
\name~adopts cutting-edge practice in agile software development and cloud computing to build a totally unique capability for neuroscience. By adopting containers, which encapsulate all software dependencies, and serverless computing, we will create an automated system that runs arbitrary and sophisticated checks on submitted data. Data contributors experience no overhead: they upload a data set via file transfer or Web service and the system automatically runs the continuous integration suites. Experiment designers write simple programs, e.g. scripts, or complex workflows to express their constraints and continuous integration enforces these automatically and at scale.

\vspace{-3pt}

