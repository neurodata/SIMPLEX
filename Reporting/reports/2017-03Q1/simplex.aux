\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\pgfsyspdfmark {pgfid1}{4736286}{47362867}
\providecommand\tcolorbox@label[2]{}
\pgfsyspdfmark {pgfid2}{4736286}{47362867}
\@writefile{toc}{\contentsline {section}{\numberline {1}Bibliography}{2}{section.1}}
\pgfsyspdfmark {pgfid4}{4736286}{47362867}
\@writefile{toc}{\contentsline {section}{\numberline {2}Statistical Theory and Methods}{3}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}LOL}{3}{subsection.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  LOL outperforms other linear classifiers in a wide variety of settings, including those for which we have proven LOL should (rows 1 and 3), and those beyond our current theoretical grasp (rows 2, 4, and 5). This is true regardless of the dimensionality into which we embed. \relax }}{3}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:LOL}{{1}{3}{LOL outperforms other linear classifiers in a wide variety of settings, including those for which we have proven LOL should (rows 1 and 3), and those beyond our current theoretical grasp (rows 2, 4, and 5). This is true regardless of the dimensionality into which we embed. \relax }{figure.caption.1}{}}
\pgfsyspdfmark {pgfid6}{4736286}{47362867}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Top: LOL outperforms essentially all other methods for essentially all problems and number of dimensions to embed into. Bottom: LOL (green) and LAL (brown, using random projections to approximate) for both fully in memory (IE) and semi-external memory (E) implementations. Magenta compares performance to our scalable implementation of eigenfaces on the same problem. \relax }}{4}{figure.caption.2}}
\newlabel{fig:lolall}{{2}{4}{Top: LOL outperforms essentially all other methods for essentially all problems and number of dimensions to embed into. Bottom: LOL (green) and LAL (brown, using random projections to approximate) for both fully in memory (IE) and semi-external memory (E) implementations. Magenta compares performance to our scalable implementation of eigenfaces on the same problem. \relax }{figure.caption.2}{}}
\pgfsyspdfmark {pgfid7}{4736286}{47362867}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}meda}{5}{subsection.2.2}}
\pgfsyspdfmark {pgfid9}{4736286}{47362867}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A one dimensional heatmap for each of the $\qopname  \relax o{log}_{10}$ transformed feature columns of the Kristina15 synaptome dataset. Colors correspond to the count of data points falling in to each bin. Scott's binning streategy is used which, in this case, yields 105 bins of equal width $w = 0.05$. Label colors and groups starting from the bottom: \leavevmode {\color  {jgreen}Synap1\_F0 - Synapo\_F0 (green, excitatory)}, \leavevmode {\color  {jred}gad\_F0 - GABABR\_F0 (red, inhibatory)}, \leavevmode {\color  {jblue}Vglut3\_F0 - DAPI\_F0 (blue, other)}. \relax }}{6}{figure.caption.3}}
\newlabel{fig:meda201701}{{3}{6}{A one dimensional heatmap for each of the $\log _{10}$ transformed feature columns of the Kristina15 synaptome dataset. Colors correspond to the count of data points falling in to each bin. Scott's binning streategy is used which, in this case, yields 105 bins of equal width $w = 0.05$. Label colors and groups starting from the bottom: \textcolor {jgreen}{Synap1\_F0 - Synapo\_F0 (green, excitatory)}, \textcolor {jred}{gad\_F0 - GABABR\_F0 (red, inhibatory)}, \textcolor {jblue}{Vglut3\_F0 - DAPI\_F0 (blue, other)}. \relax }{figure.caption.3}{}}
\pgfsyspdfmark {pgfid11}{4736286}{47362867}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Left: A dendrogram showing the results of hierarchical mclust. The splits are constrained to be binary and branch sizes show relative cluster sizes. Right: A stacked level means plot showing for each node in the dendrogram the feature means.\relax }}{7}{figure.caption.4}}
\newlabel{fig:meda201702}{{4}{7}{Left: A dendrogram showing the results of hierarchical mclust. The splits are constrained to be binary and branch sizes show relative cluster sizes. Right: A stacked level means plot showing for each node in the dendrogram the feature means.\relax }{figure.caption.4}{}}
\pgfsyspdfmark {pgfid13}{4736286}{47362867}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Multiscale Generalized Correlation (MGC)}{8}{subsection.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The average power advantage of MGC versus popular benchmarks, throughout $20$ different linear and nonlinear high-dimensional dependencies.\relax }}{8}{figure.caption.5}}
\newlabel{fig:mgcall}{{5}{8}{The average power advantage of MGC versus popular benchmarks, throughout $20$ different linear and nonlinear high-dimensional dependencies.\relax }{figure.caption.5}{}}
\pgfsyspdfmark {pgfid15}{4736286}{47362867}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Network Dependence Test via Diffusion Maps and MGC}{9}{subsection.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Power comparison for all possible combinations of metrics and correlation measure, under the stochastic block model with three blocks. \texttt  {MGC} with the diffusion maps (DM) yields the best power, comparing to using other metrics like adjacency matrix (AM), latent factors (LF), and other test statistics like distance correlation (mcorr), Heller-Heller-Gorfine (HHG) test, or Fosdick and Hoff (FH) method.\relax }}{9}{figure.caption.6}}
\newlabel{fig:threeSBM201701}{{6}{9}{Power comparison for all possible combinations of metrics and correlation measure, under the stochastic block model with three blocks. \texttt {MGC} with the diffusion maps (DM) yields the best power, comparing to using other metrics like adjacency matrix (AM), latent factors (LF), and other test statistics like distance correlation (mcorr), Heller-Heller-Gorfine (HHG) test, or Fosdick and Hoff (FH) method.\relax }{figure.caption.6}{}}
\pgfsyspdfmark {pgfid17}{4736286}{47362867}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The power curve with respect to increasing number of vertices for the two-graph dependency testing simulation. The proposed approach quickly attains perfect power at a very small vertex size, while other benchmarks often require a much larger graph for perfect testing. \relax }}{10}{figure.caption.7}}
\newlabel{fig:threeSBM201703}{{7}{10}{The power curve with respect to increasing number of vertices for the two-graph dependency testing simulation. The proposed approach quickly attains perfect power at a very small vertex size, while other benchmarks often require a much larger graph for perfect testing. \relax }{figure.caption.7}{}}
\pgfsyspdfmark {pgfid19}{4736286}{47362867}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Randomer Forest}{11}{subsection.2.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  Classification performances of RF, RerF, RR-RF, and XGBoost on 119 benchmark datasets. \relax }}{11}{figure.caption.8}}
\newlabel{fig:RefF3}{{8}{11}{Classification performances of RF, RerF, RR-RF, and XGBoost on 119 benchmark datasets. \relax }{figure.caption.8}{}}
\pgfsyspdfmark {pgfid21}{4736286}{47362867}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Error rate of RF, RerF, and RR-RF on the "Orthant" dataset as a function of $n$, the number of training samples, for three values of $p$. The results indicate that there is no significant difference in performance between RerF and RF, while RR-RF performs significantly worse across all settings.\relax }}{12}{figure.caption.9}}
\newlabel{fig:name}{{9}{12}{Error rate of RF, RerF, and RR-RF on the "Orthant" dataset as a function of $n$, the number of training samples, for three values of $p$. The results indicate that there is no significant difference in performance between RerF and RF, while RR-RF performs significantly worse across all settings.\relax }{figure.caption.9}{}}
\pgfsyspdfmark {pgfid23}{4736286}{47362867}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  Theoretical performances of simplified RF and RerF models on a simple two-dimensional binary classification problem. In the top row, the two classes are distributed according to normal distributions having means that differ only in the first dimension and both having identity covariances. These distributions are shown in the left and middle panels. The right-most panel shows the theoretical error rate as a function of the number of training samples for RF and RerF. The Bayes optimal error rate is also shown for reference. The middle and bottom rows are the same as the top row, except the distributions have been rotated by 22.5째 and 45째 respectively. In all three cases, the RerF classifier outperforms RF for all training set sizes. \relax }}{13}{figure.caption.10}}
\newlabel{fig:RerF}{{10}{13}{Theoretical performances of simplified RF and RerF models on a simple two-dimensional binary classification problem. In the top row, the two classes are distributed according to normal distributions having means that differ only in the first dimension and both having identity covariances. These distributions are shown in the left and middle panels. The right-most panel shows the theoretical error rate as a function of the number of training samples for RF and RerF. The Bayes optimal error rate is also shown for reference. The middle and bottom rows are the same as the top row, except the distributions have been rotated by 22.5째 and 45째 respectively. In all three cases, the RerF classifier outperforms RF for all training set sizes. \relax }{figure.caption.10}{}}
\pgfsyspdfmark {pgfid25}{4736286}{47362867}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Non-Parametric Shape Clustering}{14}{subsection.2.6}}
\newlabel{fig:nonpar57}{{\caption@xref {fig:nonpar57}{ on input line 40}}{14}{Non-Parametric Shape Clustering}{figure.caption.11}{}}
\newlabel{sub@fig:nonpar57}{{}{14}{Non-Parametric Shape Clustering}{figure.caption.11}{}}
\newlabel{fig:nonpar357}{{\caption@xref {fig:nonpar357}{ on input line 47}}{14}{Non-Parametric Shape Clustering}{figure.caption.11}{}}
\newlabel{sub@fig:nonpar357}{{a}{14}{Non-Parametric Shape Clustering}{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces  MNIST handwritten digits and classification error results. \relax }}{14}{figure.caption.11}}
\newlabel{fig:nonpar}{{11}{14}{MNIST handwritten digits and classification error results. \relax }{figure.caption.11}{}}
\pgfsyspdfmark {pgfid27}{4736286}{47362867}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces  Distribution of test statistic $T\equiv \genfrac  {}{}{}1{n m }{n+m}\mathcal  {E}(A,B)$ for an ensamble obtained from two distributions: $A \mathrel {\mathop {\sim }\limits ^{iid}} \mathcal  {N}(\mu _A,\sigma _A^2)$ and $B \mathrel {\mathop {\sim }\limits ^{iid}} \mathcal  {N}(\mu _B, \sigma _B^2)$, where $|A|=n$ and $|B|=m$. Blue histogram: $\mu _A = \mu _B = 0$ and $\sigma _A = \sigma _B = 1$; Red histogram: $\mu _A = - \mu _B = 1$ and $\sigma _A = \sigma _B = 1$; Green histogram: $\mu _A = \mu _B = 0$, $\sigma _A = 1$ and $\sigma _B = 1.5$. \relax }}{15}{figure.caption.12}}
\newlabel{fig:nonpar}{{12}{15}{Distribution of test statistic $T\equiv \tfrac {n m }{n+m}\mathcal {E}(A,B)$ for an ensamble obtained from two distributions: $A \stackrel {iid}{\sim } \mathcal {N}(\mu _A,\sigma _A^2)$ and $B \stackrel {iid}{\sim } \mathcal {N}(\mu _B, \sigma _B^2)$, where $|A|=n$ and $|B|=m$. Blue histogram: $\mu _A = \mu _B = 0$ and $\sigma _A = \sigma _B = 1$; Red histogram: $\mu _A = - \mu _B = 1$ and $\sigma _A = \sigma _B = 1$; Green histogram: $\mu _A = \mu _B = 0$, $\sigma _A = 1$ and $\sigma _B = 1.5$. \relax }{figure.caption.12}{}}
\pgfsyspdfmark {pgfid29}{4736286}{47362867}
\newlabel{eq:energy}{{\textup  {(4)}}{16}{Non-Parametric Shape Clustering}{equation.2.4}{}}
\newlabel{eq:opt}{{\textup  {(5)}}{16}{Non-Parametric Shape Clustering}{equation.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces  Two dimensional datasets and the objective function in \ref  {eq:opt} as a function of $n$, where $n$ is the number of shuffled points from its correct class to the wrong class. Blue dots are for \ref  {eq:energy} and red dots for $k$-means. A good function must be monotonically decreasing. We can clearly see that \ref  {eq:energy} is way more powerful than $k$-means. \relax }}{16}{figure.caption.13}}
\newlabel{fig:plots}{{13}{16}{Two dimensional datasets and the objective function in \ref {eq:opt} as a function of $n$, where $n$ is the number of shuffled points from its correct class to the wrong class. Blue dots are for \ref {eq:energy} and red dots for $k$-means. A good function must be monotonically decreasing. We can clearly see that \ref {eq:energy} is way more powerful than $k$-means. \relax }{figure.caption.13}{}}
\pgfsyspdfmark {pgfid30}{4736286}{47362867}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Discriminability}{17}{subsection.2.7}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Cluster samples through maximizing discriminability. \relax }}{17}{algorithm.1}}
\newlabel{alg:dcl}{{1}{17}{Cluster samples through maximizing discriminability. \relax }{algorithm.1}{}}
\pgfsyspdfmark {pgfid31}{4736286}{47362867}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Compute discriminability estimate $\mathaccentV {hat}75E{D}$. \relax }}{18}{algorithm.2}}
\newlabel{alg:dhat}{{2}{18}{Compute discriminability estimate $\hat {D}$. \relax }{algorithm.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces The function returns a p-value for testing the null hypothesis that $D = 0.5$. \relax }}{18}{algorithm.3}}
\newlabel{alg:ost}{{3}{18}{The function returns a p-value for testing the null hypothesis that $D = 0.5$. \relax }{algorithm.3}{}}
\pgfsyspdfmark {pgfid32}{4736286}{47362867}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces The function returns a p-value for testing the null hypothesis that $D(\boldsymbol  {\psi }_1)=D(\boldsymbol  {\psi }_2)$. \relax }}{19}{algorithm.4}}
\newlabel{alg:tst}{{4}{19}{The function returns a p-value for testing the null hypothesis that $D(\bpsi _1)=D(\bpsi _2)$. \relax }{algorithm.4}{}}
\pgfsyspdfmark {pgfid34}{4736286}{47362867}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}Joint Embedding}{20}{subsection.2.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces The left panel shows the graph derived from a typical subject. There is much more neural connectivity within each hemisphere. The right panel shows the rank one matrix $\mathaccentV {hat}75E{h}_6^T\mathaccentV {hat}75E{h}_6$, which has positive connectivity within each hemisphere, but negative connectivity across hemispheres.\relax }}{20}{figure.caption.14}}
\newlabel{fig:cci1}{{14}{20}{The left panel shows the graph derived from a typical subject. There is much more neural connectivity within each hemisphere. The right panel shows the rank one matrix $\hat {h}_6^T\hat {h}_6$, which has positive connectivity within each hemisphere, but negative connectivity across hemispheres.\relax }{figure.caption.14}{}}
\pgfsyspdfmark {pgfid36}{4736286}{47362867}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.9}Robust Law of Large Graphs}{21}{subsection.2.9}}
\pgfsyspdfmark {pgfid37}{4736286}{47362867}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces  {\bf  Comparison of MSE of the four estimators for the desikan atlases at three sample sizes based on m2g and ndmg pipelines.} {\bf  1. MLE (horizontal solid line) vs MLqE (horizontal dotted line):} ML$q$E outperforms MLE since robust estimators are always preferred in practice; {\bf  2. MLE (horizontal solid line) vs MLE\_ASE (dashed line):} MLE\_ASE wins the bias-variance tradeoff when embedded into a proper dimension; {\bf  3. MLqE (horizontal dotted line) vs ML$q$E\_ASE (dashed dotted line):} ML$q$E\_ASE wins the bias-variance tradeoff when embedded into a proper dimension; {\bf  4. ML$q$E\_ASE (dashed dotted line) vs MLE\_ASE (dashed line):} MLqE\_ASE is better, since it inherits the robustness from ML$q$E. And the square and circle represent the dimensions selected by the Zhu and Ghodsi method. We can see it does a pretty good job. But more importantly, a wide range of dimensions could lead to an improvement. \relax }}{22}{figure.caption.15}}
\newlabel{fig:robDim}{{15}{22}{{\bf Comparison of MSE of the four estimators for the desikan atlases at three sample sizes based on m2g and ndmg pipelines.} {\bf 1. MLE (horizontal solid line) vs MLqE (horizontal dotted line):} ML$q$E outperforms MLE since robust estimators are always preferred in practice; {\bf 2. MLE (horizontal solid line) vs MLE\_ASE (dashed line):} MLE\_ASE wins the bias-variance tradeoff when embedded into a proper dimension; {\bf 3. MLqE (horizontal dotted line) vs ML$q$E\_ASE (dashed dotted line):} ML$q$E\_ASE wins the bias-variance tradeoff when embedded into a proper dimension; {\bf 4. ML$q$E\_ASE (dashed dotted line) vs MLE\_ASE (dashed line):} MLqE\_ASE is better, since it inherits the robustness from ML$q$E. And the square and circle represent the dimensions selected by the Zhu and Ghodsi method. We can see it does a pretty good job. But more importantly, a wide range of dimensions could lead to an improvement. \relax }{figure.caption.15}{}}
\pgfsyspdfmark {pgfid38}{4736286}{47362867}
\pgfsyspdfmark {pgfid39}{4736286}{47362867}
\pgfsyspdfmark {pgfid41}{4736286}{47362867}
\@writefile{toc}{\contentsline {section}{\numberline {3}Scalable Algorithm Implementations}{25}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}FlashX}{25}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces  Normalized runtime of FlashR vs. Revolution R when executing R implementations of machine learning primitives on a dataset with one million data points and 1000 features on a large parallel machine with 48 CPU cores. FlashR outperforms Revolution R in all computations. When the computation gets more complex, the speed advantage of FlashR over Revolution R gets larger. \relax }}{25}{figure.caption.16}}
\newlabel{fig:flashr}{{16}{25}{Normalized runtime of FlashR vs. Revolution R when executing R implementations of machine learning primitives on a dataset with one million data points and 1000 features on a large parallel machine with 48 CPU cores. FlashR outperforms Revolution R in all computations. When the computation gets more complex, the speed advantage of FlashR over Revolution R gets larger. \relax }{figure.caption.16}{}}
\pgfsyspdfmark {pgfid42}{4736286}{47362867}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The runtime and memory consumption of FlashR on the billion-scale datasets on the 48 CPU core machine. The runtime of iterative algorithms is measured when the algorithms converge. We run PageRank on the PageGraph dataset, run k-means on PageGraph-32ev and the remaining algorithms on Criteo.\relax }}{26}{table.caption.17}}
\newlabel{tbl:scale}{{1}{26}{The runtime and memory consumption of FlashR on the billion-scale datasets on the 48 CPU core machine. The runtime of iterative algorithms is measured when the algorithms converge. We run PageRank on the PageGraph dataset, run k-means on PageGraph-32ev and the remaining algorithms on Criteo.\relax }{table.caption.17}{}}
\pgfsyspdfmark {pgfid44}{4736286}{47362867}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Left: the in-degree distribution of the Page graph. Right: a heatmap of the distribution of the vertices of the page graph in a two-dimension space; the coordinates of the vertices in the two-dimension space is determined by the first left and right singular vectors.\relax }}{27}{figure.caption.18}}
\newlabel{fig:FlashX}{{17}{27}{Left: the in-degree distribution of the Page graph. Right: a heatmap of the distribution of the vertices of the page graph in a two-dimension space; the coordinates of the vertices in the two-dimension space is determined by the first left and right singular vectors.\relax }{figure.caption.18}{}}
\pgfsyspdfmark {pgfid45}{4736286}{47362867}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}ndstore}{28}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}ndingest}{28}{subsubsection.3.2.1}}
\pgfsyspdfmark {pgfid47}{4736286}{47362867}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}ndviz}{29}{subsection.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces NeuroDataViz, powered by Neuroglancer with pre-computed meshes displayed in 3-d. Data from Harris et al. ``A resource from 3D electron microscopy of hippocampal neuropil for user training and tool development,'' Nature Scientific Data 2015.\relax }}{29}{figure.caption.19}}
\newlabel{fig:name}{{18}{29}{NeuroDataViz, powered by Neuroglancer with pre-computed meshes displayed in 3-d. Data from Harris et al. ``A resource from 3D electron microscopy of hippocampal neuropil for user training and tool development,'' Nature Scientific Data 2015.\relax }{figure.caption.19}{}}
\pgfsyspdfmark {pgfid49}{4736286}{47362867}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}knor: K-means NUMA Optimized Routines}{30}{subsection.3.4}}
\pgfsyspdfmark {pgfid50}{4736286}{47362867}
\newlabel{fig:perf-rmvm}{{19a}{31}{Per iteration time elapsed of each routine.\relax }{figure.caption.20}{}}
\newlabel{sub@fig:perf-rmvm}{{a}{31}{Per iteration time elapsed of each routine.\relax }{figure.caption.20}{}}
\newlabel{fig:mem-rmvm}{{19b}{31}{Memory consumption of each routine.\relax }{figure.caption.20}{}}
\newlabel{sub@fig:mem-rmvm}{{b}{31}{Memory consumption of each routine.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Speed and Memory comparison on randomly generated datasets (i) RM$_{856M}$, a 856 Million X 16 dataset of size 103GB and (ii) RM$_{1B}$, a 1 Billion X 32 dataset of size 251 GB and (iii) RM$_{2B}$, a 2 Billion X 64 dataset of size 1.1 TB. Turi is unable to run on RM$_{1B}$ on our machine and only SEM routines are able to run on RU$_{2B}$ on our machine with 48 Cores and 1 TB of RAM.\relax }}{31}{figure.caption.20}}
\newlabel{fig:rmvm}{{19}{31}{Speed and Memory comparison on randomly generated datasets (i) RM$_{856M}$, a 856 Million X 16 dataset of size 103GB and (ii) RM$_{1B}$, a 1 Billion X 32 dataset of size 251 GB and (iii) RM$_{2B}$, a 2 Billion X 64 dataset of size 1.1 TB. Turi is unable to run on RM$_{1B}$ on our machine and only SEM routines are able to run on RU$_{2B}$ on our machine with 48 Cores and 1 TB of RAM.\relax }{figure.caption.20}{}}
\pgfsyspdfmark {pgfid52}{4736286}{47362867}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}ndreg}{32}{subsection.3.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces  Coronal slices from image volumes. From left to right: CLARITY before LDDMM, CLARITY after LDDMM, ARA, checkerboard composite of ARA and CLARITY after LDDMM. \relax }}{32}{figure.caption.21}}
\newlabel{fig:ndregAiley}{{20}{32}{Coronal slices from image volumes. From left to right: CLARITY before LDDMM, CLARITY after LDDMM, ARA, checkerboard composite of ARA and CLARITY after LDDMM. \relax }{figure.caption.21}{}}
\pgfsyspdfmark {pgfid54}{4736286}{47362867}
\newlabel{fig:clarityCoronalSSD}{{21a}{33}{SSD-LDDMM\relax }{figure.caption.22}{}}
\newlabel{sub@fig:clarityCoronalSSD}{{a}{33}{SSD-LDDMM\relax }{figure.caption.22}{}}
\newlabel{fig:clarityCoronalMask}{{21b}{33}{Mask-LDDMM\relax }{figure.caption.22}{}}
\newlabel{sub@fig:clarityCoronalMask}{{b}{33}{Mask-LDDMM\relax }{figure.caption.22}{}}
\newlabel{fig:clarityCoronalMI}{{21c}{33}{MI-LDDMM\relax }{figure.caption.22}{}}
\newlabel{sub@fig:clarityCoronalMI}{{c}{33}{MI-LDDMM\relax }{figure.caption.22}{}}
\newlabel{fig:lmkError}{{21d}{33}{Landmark Error\relax }{figure.caption.22}{}}
\newlabel{sub@fig:lmkError}{{d}{33}{Landmark Error\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Comparison of SSD-LDDMM (\subref  {fig:clarityCoronalSSD}), Mask-LDDMM (\subref  {fig:clarityCoronalMask}) and MI-LDDMM (\subref  {fig:clarityCoronalMI}) registration for a CLARITY mouse brain. Panes (\subref  {fig:clarityCoronalSSD}-\subref  {fig:clarityCoronalMI}) have an ARA coronal slice on the left juxtaposed to the corresponding aligned CLARITY slice on the right. Green arrows point out that the corpus callosum is misaligned by SSD-LDDMM but aligned correctly by MI matching. Red arrows show that SSD-LDDMM distorts bright regions. Fiducial landmarks were placed throughout the corpus callosum, and midbrain of the acquired volumes. Pane (\subref  {fig:lmkError}) compares mean errors between the deformed CLARITY and ARA landmarks after registration. \relax }}{33}{figure.caption.22}}
\newlabel{fig:comparison}{{21}{33}{Comparison of SSD-LDDMM (\subref {fig:clarityCoronalSSD}), Mask-LDDMM (\subref {fig:clarityCoronalMask}) and MI-LDDMM (\subref {fig:clarityCoronalMI}) registration for a CLARITY mouse brain. Panes (\subref {fig:clarityCoronalSSD}-\subref {fig:clarityCoronalMI}) have an ARA coronal slice on the left juxtaposed to the corresponding aligned CLARITY slice on the right. Green arrows point out that the corpus callosum is misaligned by SSD-LDDMM but aligned correctly by MI matching. Red arrows show that SSD-LDDMM distorts bright regions. Fiducial landmarks were placed throughout the corpus callosum, and midbrain of the acquired volumes. Pane (\subref {fig:lmkError}) compares mean errors between the deformed CLARITY and ARA landmarks after registration. \relax }{figure.caption.22}{}}
\pgfsyspdfmark {pgfid56}{4736286}{47362867}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Waxholm rat atlas labels overlaid on sagittal slices of iDISCO rat brain\relax }}{34}{figure.caption.23}}
\newlabel{fig:idisco}{{22}{34}{Waxholm rat atlas labels overlaid on sagittal slices of iDISCO rat brain\relax }{figure.caption.23}{}}
\pgfsyspdfmark {pgfid57}{4736286}{47362867}
\@writefile{toc}{\contentsline {section}{\numberline {4}Data: What's in the Cloud}{35}{section.4}}
\newlabel{tab:image}{{\caption@xref {tab:image}{ on input line 36}}{35}{Data: What's in the Cloud}{table.caption.24}{}}
\pgfsyspdfmark {pgfid58}{4736286}{47362867}
\@writefile{toc}{\contentsline {section}{\numberline {5}Data: What's in the Cloud}{36}{section.5}}
\newlabel{tab:image}{{\caption@xref {tab:image}{ on input line 36}}{36}{Data: What's in the Cloud}{table.caption.25}{}}
\pgfsyspdfmark {pgfid61}{4736286}{47362867}
\@writefile{toc}{\contentsline {section}{\numberline {6}Scientific Pipelines: Infrastructure \& Dataset Specific Progress}{37}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Science in the Cloud}{37}{subsection.6.1}}
\pgfsyspdfmark {pgfid62}{4736286}{47362867}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces  States of the demo notebook in the cloud. A) A Jupyter notebook displaying descriptions and code snippets to be run for both connectome estimation and summary statistic computation. B) After running connectome generation an adjacency matrix will appear to provide a visualization. C) Summary statistic computation calculates several graph features and plots them in a multipanel figure. \relax }}{38}{figure.caption.26}}
\newlabel{fig:sic}{{23}{38}{States of the demo notebook in the cloud. A) A Jupyter notebook displaying descriptions and code snippets to be run for both connectome estimation and summary statistic computation. B) After running connectome generation an adjacency matrix will appear to provide a visualization. C) Summary statistic computation calculates several graph features and plots them in a multipanel figure. \relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces  Excerpted table from \citeman {kiar2016sic}. \relax }}{38}{figure.caption.27}}
\newlabel{fig:sicTab}{{24}{38}{Excerpted table from \protect \citeman {kiar2016sic}. \relax }{figure.caption.27}{}}
\pgfsyspdfmark {pgfid64}{4736286}{47362867}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}ndmg}{39}{subsection.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Quality Control of Connectomes\relax }}{39}{figure.caption.28}}
\newlabel{fig:ndmgqc}{{25}{39}{Quality Control of Connectomes\relax }{figure.caption.28}{}}
\pgfsyspdfmark {pgfid66}{4736286}{47362867}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces \textbf  {Multi-Study Mean Connectomes}. Looking at 70-nodes graphs built upon the Desikan parcellation, we have computed the mean connectome from a variety of processed datasets. We then computed a weighted mean and standard deviation of the mean connectomes to produce the largest known population-level connectome to-date, consisting of 2861 sessions. As expected, ipsi-lateral connectivity is consistently more dense than contra-lateral connectivity. Similarly, the standard deviation connectome, which highlights edges that are more highly variable, shows higher ipsi-lateral density. This suggests that not only are same-hemisphere connections more likely to occur, but they have a higher variance than across-hemisphere connections, as well.\relax }}{40}{figure.caption.29}}
\newlabel{fig:mean}{{26}{40}{\textbf {Multi-Study Mean Connectomes}. Looking at 70-nodes graphs built upon the Desikan parcellation, we have computed the mean connectome from a variety of processed datasets. We then computed a weighted mean and standard deviation of the mean connectomes to produce the largest known population-level connectome to-date, consisting of 2861 sessions. As expected, ipsi-lateral connectivity is consistently more dense than contra-lateral connectivity. Similarly, the standard deviation connectome, which highlights edges that are more highly variable, shows higher ipsi-lateral density. This suggests that not only are same-hemisphere connections more likely to occur, but they have a higher variance than across-hemisphere connections, as well.\relax }{figure.caption.29}{}}
\pgfsyspdfmark {pgfid67}{4736286}{47362867}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}CLARITY}{41}{subsection.6.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}A low-latency pipeline for processing CLARITY data in the cloud}{41}{subsubsection.6.3.1}}
\pgfsyspdfmark {pgfid68}{4736286}{47362867}
\@writefile{toc}{\contentsline {section}{\numberline {7}Reference Datasets}{42}{section.7}}
\newlabel{LastPage}{{}{42}{}{page.42}{}}
\xdef\lastpage@lastpage{42}
\xdef\lastpage@lastpageHy{42}
